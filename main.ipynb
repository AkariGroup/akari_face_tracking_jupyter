{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fd8237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "from queue import Queue\n",
    "import cv2\n",
    "import depthai as dai\n",
    "import numpy as np\n",
    "import blobconverter\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "from akari_client import AkariClient\n",
    "from utils.priorbox import PriorBox\n",
    "from utils.utils import draw\n",
    "\n",
    "pan_target_angle = 0.0\n",
    "tilt_target_angle = 0.0\n",
    "\n",
    "# resize input to smaller size for faster inference\n",
    "NN_WIDTH, NN_HEIGHT = 160, 120\n",
    "VIDEO_WIDTH, VIDEO_HEIGHT = 640, 480\n",
    "\n",
    "\n",
    "# 顔追従するクラス\n",
    "class FaceTracker:\n",
    "    def __init__(self) -> None:\n",
    "        global pan_target_angle\n",
    "        global tilt_target_angle\n",
    "\n",
    "        # AkariClientのインスタンスを作成する\n",
    "        self.akari = AkariClient()\n",
    "        self.joints = self.akari.joints\n",
    "\n",
    "        self._default_x = 0\n",
    "        self._default_y = 0\n",
    "\n",
    "        # サーボトルクON\n",
    "        self.joints.enable_all_servo()\n",
    "        # モータ速度設定\n",
    "        self.joints.set_joint_velocities(pan=10, tilt=10)\n",
    "        # モータ加速度設定\n",
    "        self.joints.set_joint_accelerations(pan=30, tilt=30)\n",
    "\n",
    "        # Initialize motor position\n",
    "        self.joints.move_joint_positions(pan=0, tilt=0)\n",
    "        while True:\n",
    "            if (\n",
    "                abs(self.joints.get_joint_positions()[\"pan\"] - self._default_x) <= 0.087\n",
    "                and abs(self.joints.get_joint_positions()[\"tilt\"] - self._default_y)\n",
    "                <= 0.087\n",
    "            ):\n",
    "                break\n",
    "        self.currentMotorAngle = self.joints.get_joint_positions()\n",
    "\n",
    "        # Dynamixel Input Value\n",
    "        pan_target_angle = self.currentMotorAngle[\"pan\"]\n",
    "        tilt_target_angle = self.currentMotorAngle[\"tilt\"]\n",
    "\n",
    "    def _tracker(self) -> None:\n",
    "        global pan_target_angle\n",
    "        global tilt_target_angle\n",
    "        while True:\n",
    "            self.joints.move_joint_positions(\n",
    "                pan=pan_target_angle, tilt=tilt_target_angle\n",
    "            )\n",
    "            time.sleep(0.01)\n",
    "\n",
    "\n",
    "# モータ動作角を生成するクラス\n",
    "class DirectionUpdater:\n",
    "    _H_PIX_WIDTH = VIDEO_WIDTH\n",
    "    _H_PIX_HEIGHT = VIDEO_HEIGHT\n",
    "    _PAN_THRESHOLD = 0.1\n",
    "    _TILT_THRESHOLD = 0.1\n",
    "    _pan_dev = 0\n",
    "    _tilt_dev = 0\n",
    "    # モータゲインの最大幅。追従性の最大はここで変更\n",
    "    _MAX_PAN_GAIN = 0.1\n",
    "    _MAX_TILT_GAIN = 0.1\n",
    "    # モータゲインの最小幅。追従性の最小はここで変更\n",
    "    _MIN_PAN_GAIN = 0.07\n",
    "    _MIN_TILT_GAIN = 0.07\n",
    "    # 顔の距離によってモータゲインを変化させる係数。上げると早い動きについていきやすいが、オーバーシュートしやすくなる。\n",
    "    _GAIN_COEF_PAN = 0.0001\n",
    "    _GAIN_COEF_TILT = 0.0001\n",
    "\n",
    "    _pan_p_gain = _MIN_PAN_GAIN\n",
    "    _tilt_p_gain = _MIN_TILT_GAIN\n",
    "\n",
    "    _PAN_POS_MAX = 1.047\n",
    "    _PAN_POS_MIN = -1.047\n",
    "    _TILT_POS_MAX = 0.523\n",
    "    _TILT_POS_MIN = -0.523\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        global prev_time\n",
    "        global cur_time\n",
    "        self._face_x = 0\n",
    "        self._face_y = 0\n",
    "        self._face_width = 0\n",
    "        self._face_height = 0\n",
    "        self._old_face_x: float = 0\n",
    "        self._old_face_y: float = 0\n",
    "\n",
    "    def _calc_p_gain(self) -> None:\n",
    "        self._pan_p_gain = self._GAIN_COEF_PAN * self._face_width\n",
    "        if self._pan_p_gain > self._MAX_PAN_GAIN:\n",
    "            self._pan_p_gain = self._MAX_PAN_GAIN\n",
    "        elif self._pan_p_gain < self._MIN_PAN_GAIN:\n",
    "            self._pan_p_gain = self._MIN_PAN_GAIN\n",
    "        self._tilt_p_gain = self._GAIN_COEF_TILT * self._face_width\n",
    "        if self._tilt_p_gain > self._MAX_TILT_GAIN:\n",
    "            self._tilt_p_gain = self._MAX_TILT_GAIN\n",
    "        elif self._tilt_p_gain < self._MIN_TILT_GAIN:\n",
    "            self._tilt_p_gain = self._MIN_TILT_GAIN\n",
    "\n",
    "    def _face_info_cb(self, q_detection) -> None:\n",
    "        while True:\n",
    "            self.detections = q_detection.get()\n",
    "\n",
    "            self._face_x = self.detections[0]\n",
    "            self._face_y = self.detections[1]\n",
    "\n",
    "            self._face_width = self.detections[2]\n",
    "            self._face_height = self.detections[3]\n",
    "\n",
    "            self._set_goal_pos(\n",
    "                self._face_x + self._face_width / 2,\n",
    "                self._face_y + self._face_height / 2,\n",
    "            )\n",
    "            self._calc_p_gain()\n",
    "\n",
    "    def _set_goal_pos(self, face_x, face_y) -> None:\n",
    "        global pan_target_angle\n",
    "        global tilt_target_angle\n",
    "        if face_x >= 1000:\n",
    "            face_x = 0\n",
    "        if face_y >= 1000:\n",
    "            face_y = 0\n",
    "        pan_error = -(face_x + self._pan_dev - self._H_PIX_WIDTH / 2.0) / (\n",
    "            self._H_PIX_WIDTH / 2.0\n",
    "        )  # -1 ~ 1\n",
    "        tilt_error = -(face_y + self._tilt_dev - self._H_PIX_HEIGHT / 2.0) / (\n",
    "            self._H_PIX_HEIGHT / 2.0\n",
    "        )  # -1 ~ 1\n",
    "\n",
    "        if abs(pan_error) > self._PAN_THRESHOLD and not (face_x == self._old_face_x):\n",
    "            pan_target_angle += self._pan_p_gain * pan_error\n",
    "        if pan_target_angle < self._PAN_POS_MIN:\n",
    "            pan_target_angle = self._PAN_POS_MIN\n",
    "        elif pan_target_angle > self._PAN_POS_MAX:\n",
    "            pan_target_angle = self._PAN_POS_MAX\n",
    "        if abs(tilt_error) > self._TILT_THRESHOLD and not (face_y == self._old_face_y):\n",
    "            tilt_target_angle += self._tilt_p_gain * tilt_error\n",
    "        if tilt_target_angle < self._TILT_POS_MIN:\n",
    "            tilt_target_angle = self._TILT_POS_MIN\n",
    "        elif tilt_target_angle > self._TILT_POS_MAX:\n",
    "            tilt_target_angle = self._TILT_POS_MAX\n",
    "\n",
    "        self._old_face_x = face_x\n",
    "        self._old_face_y = face_y\n",
    "\n",
    "\n",
    "# 顔認識する関数\n",
    "def FaceRecognition(q_detection) -> None:\n",
    "    confidence_thresh = 0.6\n",
    "    iou_thresh=0.3\n",
    "    keep_top_k = 750\n",
    "\n",
    "    pipeline = dai.Pipeline()\n",
    "    pipeline.setOpenVINOVersion(version=dai.OpenVINO.VERSION_2021_4)\n",
    "\n",
    "    detection_nn = pipeline.create(dai.node.NeuralNetwork)\n",
    "    detection_nn.setBlobPath(blobconverter.from_zoo(name=\"face_detection_yunet_160x120\", zoo_type=\"depthai\", shaves=6))\n",
    "    detection_nn.setNumPoolFrames(4)\n",
    "    detection_nn.input.setBlocking(False)\n",
    "    detection_nn.setNumInferenceThreads(2)\n",
    "\n",
    "    cam = pipeline.create(dai.node.ColorCamera)\n",
    "    cam.setPreviewSize(VIDEO_WIDTH, VIDEO_HEIGHT)\n",
    "    cam.setInterleaved(False)\n",
    "    cam.setFps(40)\n",
    "    cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
    "\n",
    "    manip = pipeline.create(dai.node.ImageManip)\n",
    "    manip.initialConfig.setResize(NN_WIDTH, NN_HEIGHT)\n",
    "    manip.initialConfig.setFrameType(dai.RawImgFrame.Type.BGR888p)\n",
    "    manip.inputConfig.setWaitForMessage(False)\n",
    "\n",
    "    xout_cam = pipeline.create(dai.node.XLinkOut)\n",
    "    xout_cam.setStreamName(\"cam\")\n",
    "\n",
    "    xout_nn = pipeline.create(dai.node.XLinkOut)\n",
    "    xout_nn.setStreamName(\"nn\")\n",
    "\n",
    "    cam.preview.link(manip.inputImage)\n",
    "    cam.preview.link(xout_cam.input)\n",
    "    manip.out.link(detection_nn.input)\n",
    "    detection_nn.out.link(xout_nn.input)\n",
    "\n",
    "\n",
    "    with dai.Device(pipeline) as device:\n",
    "        q_cam = device.getOutputQueue(\"cam\", 4, blocking=False)  # type: ignore\n",
    "        q_nn = device.getOutputQueue(name=\"nn\", maxSize=4, blocking=False)  # type: ignore\n",
    "        start_time = time.time()\n",
    "        counter = 0\n",
    "        fps = 0.0\n",
    "        layer_info_printed = False\n",
    "        display_handle=display(None, display_id=True)\n",
    "        while True:\n",
    "            in_frame = q_cam.get()\n",
    "            in_nn = q_nn.get()\n",
    "            frame = in_frame.getCvFrame()\n",
    "            conf = np.array(in_nn.getLayerFp16(\"conf\")).reshape((1076, 2))\n",
    "            iou = np.array(in_nn.getLayerFp16(\"iou\")).reshape((1076, 1))\n",
    "            loc = np.array(in_nn.getLayerFp16(\"loc\")).reshape((1076, 14))\n",
    "            pb = PriorBox(\n",
    "                input_shape=(NN_WIDTH, NN_HEIGHT),\n",
    "                output_shape=(frame.shape[1], frame.shape[0]),\n",
    "            )\n",
    "            dets = pb.decode(loc, conf, iou, confidence_thresh)\n",
    "            if dets.shape[0] > 0:\n",
    "                bboxes = dets[:, 0:4]\n",
    "                scores = dets[:, -1]\n",
    "                keep_idx = cv2.dnn.NMSBoxes(\n",
    "                    bboxes=bboxes.tolist(),\n",
    "                    scores=scores.tolist(),\n",
    "                    score_threshold=confidence_thresh,\n",
    "                    nms_threshold=iou_thresh,\n",
    "                    eta=1,\n",
    "                    top_k=keep_top_k,\n",
    "                )  # returns [box_num, class_num]\n",
    "                keep_idx = np.squeeze(keep_idx)  # [box_num, class_num] -> [box_num]\n",
    "                dets = dets[keep_idx]\n",
    "                \n",
    "                # バウンディングボックスの値をQueueに挿入する\n",
    "                q_detection.put(bboxes[0])\n",
    "                \n",
    "            if dets.shape[0] > 0:\n",
    "                if dets.ndim == 1:\n",
    "                    dets = np.expand_dims(dets, 0)\n",
    "                img_res = draw(\n",
    "                    img=frame,\n",
    "                    bboxes=dets[:, :4],\n",
    "                    landmarks=np.reshape(dets[:, 4:14], (-1, 5, 2)),\n",
    "                    scores=dets[:, -1],\n",
    "                )\n",
    "            color_black, color_white = (0, 0, 0), (255, 255, 255)\n",
    "            label_fps = \"Fps: {:.2f}\".format(fps)\n",
    "            (w1, h1), _ = cv2.getTextSize(label_fps, cv2.FONT_HERSHEY_TRIPLEX, 0.4, 1)\n",
    "            cv2.rectangle(\n",
    "                frame,\n",
    "                (0, frame.shape[0] - h1 - 6),\n",
    "                (w1 + 2, frame.shape[0]),\n",
    "                color_white,\n",
    "                -1,\n",
    "            )\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                label_fps,\n",
    "                (2, frame.shape[0] - 4),\n",
    "                cv2.FONT_HERSHEY_TRIPLEX,\n",
    "                0.4,\n",
    "                color_black,\n",
    "            )\n",
    "            rows, columns, _ = frame.shape\n",
    "            resizedFrame = cv2.resize(frame, (int(columns/2), int(rows/2)))\n",
    "            _, jpg = cv2.imencode('.jpeg', resizedFrame)\n",
    "            display_handle.update(Image(data=jpg.tobytes()))\n",
    "            counter += 1\n",
    "            if (time.time() - start_time) > 1:\n",
    "                fps = counter / (time.time() - start_time)\n",
    "                counter = 0\n",
    "                start_time = time.time()\n",
    "\n",
    "\n",
    "q_detection = Queue()\n",
    "\n",
    "face_tracker = FaceTracker()\n",
    "direction_updater = DirectionUpdater()\n",
    "\n",
    "t1 = threading.Thread(target=FaceRecognition, args=(q_detection,), daemon=True)\n",
    "t2 = threading.Thread(target=direction_updater._face_info_cb, args=(q_detection,), daemon=True)\n",
    "t3 = threading.Thread(target=face_tracker._tracker, daemon=True)\n",
    "t1.start()\n",
    "t2.start()\n",
    "t3.start()\n",
    "t1.join(timeout=1)\n",
    "t2.join(timeout=1)\n",
    "t3.join(timeout=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
